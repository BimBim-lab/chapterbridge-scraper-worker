# Scraper Worker

Automated scraping worker for ChapterBridge Dashboard. This CLI tool extracts segment (chapter/episode) lists and assets from various web sources, stores them in Cloudflare R2, and registers metadata in Supabase.

## Features

- Scrape work pages to extract segment lists (chapters/episodes)
- Scrape segment pages to extract assets (images, subtitles, text)
- Upload scraped data to Cloudflare R2
- Register metadata in Supabase database
- Job queue system for automated processing
- Configurable templates for different site structures

## Prerequisites

- Node.js v20+
- pnpm package manager
- Supabase project with the ChapterBridge schema
- Cloudflare R2 bucket

## Installation

```bash
cd scraper-worker
pnpm install
```

## Configuration

Copy the example environment file and fill in your credentials:

```bash
cp .env.example .env
```

Required environment variables:

| Variable | Description |
|----------|-------------|
| `SUPABASE_URL` | Your Supabase project URL |
| `SUPABASE_SERVICE_ROLE_KEY` | Supabase service role key (for backend operations) |
| `CLOUDFLARE_R2_ENDPOINT` | R2 S3-compatible endpoint URL |
| `CLOUDFLARE_R2_ACCESS_KEY_ID` | R2 access key ID |
| `CLOUDFLARE_R2_SECRET_ACCESS_KEY` | R2 secret access key |
| `CLOUDFLARE_R2_BUCKET` | R2 bucket name |
| `CLOUDFLARE_R2_PUBLIC_BASE_URL` | (Optional) Public URL for R2 bucket |
| `USER_AGENT` | User agent string for HTTP requests |
| `DEFAULT_RATE_LIMIT_MS` | Delay between asset downloads (default: 500) |

## Usage

### Build

```bash
pnpm build
```

### Development

```bash
pnpm dev <command>
```

### Scrape Work Page

Extract segment list from a work page:

```bash
pnpm dev scrape:work \
  --url "https://example.com/manga/title" \
  --media manhwa \
  --provider "ExampleSite" \
  --template wp-manga \
  --title "My Manga Title"
```

Options:
- `--url`: URL of the work page (required)
- `--media`: Media type - `novel`, `manhwa`, or `anime` (required)
- `--provider`: Provider/site name (required)
- `--template`: Template name to use (required)
- `--title`: Work title (required)

### Scrape Segment Page

Extract assets from a segment (chapter) page:

```bash
pnpm dev scrape:segment \
  --url "https://example.com/manga/title/chapter-1" \
  --segmentId "uuid-from-supabase" \
  --template wp-manga \
  --download
```

Options:
- `--url`: URL of the segment page (required)
- `--segmentId`: Segment UUID from Supabase (required)
- `--template`: Template name to use (required)
- `--download`: Download and upload assets to R2 (optional, default: false)

### Job Runner

Process queued jobs from `pipeline_jobs` table:

```bash
# Run continuously
pnpm dev jobs:run --poll 5000

# Run once and exit
pnpm dev jobs:run --once
```

Options:
- `--poll`: Polling interval in milliseconds (default: 5000)
- `--once`: Process one batch and exit

### List Templates

```bash
pnpm dev templates:list
```

### Validate Configuration

```bash
pnpm dev validate
```

## Output Structure

### Local Output

Scraped data is saved to the `out/` directory:
- `out/segments.json` - Segment list from work scraping
- `out/assets.json` - Asset list from segment scraping

### R2 Storage Structure

```
raw/
â””â”€â”€ <media>/                    # novel, manhwa, anime
    â””â”€â”€ <workId>/
        â””â”€â”€ <editionId>/
            â”œâ”€â”€ segments.json   # Segment list
            â””â”€â”€ <segmentType>-<number>/
                â”œâ”€â”€ assets.json     # Asset metadata
                â”œâ”€â”€ page-001.webp   # Downloaded images
                â”œâ”€â”€ page-002.webp
                â””â”€â”€ ...
```

## Templates

Templates define CSS selectors and patterns for extracting data from different site structures. They are stored in the `templates/` directory as JSON files.

### Template Structure

```json
{
  "name": "template-name",
  "description": "Description of the template",
  "selectors": {
    "chapters": ".chapter-list a",
    "chapterNumber": ".chapter-num",
    "chapterTitle": ".chapter-title",
    "image": ".reader-content img",
    "subtitle": "track[kind='subtitles']",
    "text": ".text-content p",
    "textContainer": ".novel-content"
  },
  "patterns": {
    "chapterNumberRegex": "chapter-(\\d+)",
    "imageUrlPattern": "\\.(jpg|png|webp)$"
  },
  "pagination": {
    "nextPage": ".next-page",
    "maxPages": 10
  }
}
```

### Creating a New Template

1. Create a new JSON file in `templates/` (e.g., `my-site.json`)
2. Define the CSS selectors for your target site
3. Use the template name (without .json) with the `--template` option

### Available Templates

- `wp-manga` - WordPress sites using WP-Manga plugin
- `sample` - Empty template for custom configuration

## Database Schema

The worker expects the following tables in Supabase:

- `works` - Work titles
- `editions` - Different editions/sources of a work
- `segments` - Chapters/episodes
- `assets` - Uploaded files (images, subtitles, text)
- `segment_assets` - Junction table linking segments to assets
- `pipeline_jobs` - Job queue for automated processing

## Error Handling

- All errors are logged using Pino
- Job failures are recorded in `pipeline_jobs.error`
- Failed jobs have status `failed`
- Rate limiting is applied between asset downloads

## Development

### Type Checking

```bash
pnpm typecheck
```

### Project Structure

```
scraper-worker/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ cli.ts              # CLI entry point
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ env.ts          # Environment validation
â”‚   â”‚   â””â”€â”€ templates.ts    # Template loader
â”‚   â”œâ”€â”€ extractors/
â”‚   â”‚   â”œâ”€â”€ types.ts        # TypeScript interfaces
â”‚   â”‚   â”œâ”€â”€ generic-html.ts # Generic HTML extractor
â”‚   â”‚   â”œâ”€â”€ chereads.ts     # Chereads specific extractor
â”‚   â”‚   â”œâ”€â”€ opensubtitles.ts # OpenSubtitles specific extractor
â”‚   â”‚   â””â”€â”€ wp-manga.ts     # WP-Manga specific extractor
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ supabase.ts     # Supabase operations
â”‚   â”‚   â”œâ”€â”€ r2.ts           # Cloudflare R2 operations
â”‚   â”‚   â””â”€â”€ opensubtitles-api.ts # OpenSubtitles API client
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ scrapeWork.ts   # Work scraping pipeline
â”‚   â”‚   â”œâ”€â”€ scrapeSegment.ts # Segment scraping pipeline
â”‚   â”‚   â””â”€â”€ jobRunner.ts    # Job queue processor
â”‚   â”œâ”€â”€ scripts/            # ðŸ†• Organized by media type
â”‚   â”‚   â”œâ”€â”€ novel/          # Novel/web novel scrapers
â”‚   â”‚   â”‚   â”œâ”€â”€ chereads-list.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ chereads-content.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ readnovel-list.ts
â”‚   â”‚   â”‚   â””â”€â”€ readnovel-content.ts
â”‚   â”‚   â”œâ”€â”€ anime/          # Anime subtitle scrapers
â”‚   â”‚   â”‚   â”œâ”€â”€ opensubtitles-list.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ opensubtitles-bulk-download.ts
â”‚   â”‚   â”‚   â””â”€â”€ opensubtitles-single-download.ts
â”‚   â”‚   â”œâ”€â”€ komik/          # Manhwa/comic scrapers
â”‚   â”‚   â”‚   â””â”€â”€ manhwaz-bulk-scrape.ts
â”‚   â”‚   â””â”€â”€ utils/          # Utility scripts
â”‚   â”‚       â”œâ”€â”€ sync-r2.ts
â”‚   â”‚       â””â”€â”€ fix-content-types.ts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ hash.ts         # SHA256 & file size utilities
â”‚       â””â”€â”€ logger.ts       # Pino logger configuration
â”œâ”€â”€ templates/              # Extractor templates
â”‚   â”œâ”€â”€ chereads.json
â”‚   â”œâ”€â”€ komik-manhwaz-list.json
â”‚   â”œâ”€â”€ komik-manhwaz-page.json
â”‚   â”œâ”€â”€ opensubtitles.json
â”‚   â”œâ”€â”€ readnovel.json
â”‚   â”œâ”€â”€ sample.json
â”‚   â””â”€â”€ wp-manga.json
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md     # System architecture
â”‚   â”œâ”€â”€ MIGRATION_GUIDE.md  # File reorganization guide
â”‚   â””â”€â”€ database-schema.sql # Database schema
â”œâ”€â”€ out/                    # Local output directory
â””â”€â”€ .env                    # Environment configuration
```

## License

MIT
